{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFM\n",
    "**重点**：在NFM中，特征向量进行两两交叉之后，直接进行sum pooling，将二阶交叉向量进行等权求和处理。但是直觉上来说，不同的交叉特征应该有着不同的重要性。不重要的交叉特征应该降低其权重，而重要性高的交叉特征应该提高其权重。Attention概念与该思想不谋而合，AFM作者顺势将其引入到模型之中，为每个交叉特征引入重要性权重，最终在对特征向量进行sum pooling时，利用重要性权重对二阶交叉特征进行加权求和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算公式:  \n",
    "![](./imgs/fram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "框架:  \n",
    "![](./imgs/AFM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel:\n",
    "    pass\n",
    "\n",
    "\n",
    "class NFM(BaseModel):\n",
    "    \"\"\"Instantiates the NFM Network architecture.\n",
    "\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of deep net\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to linear part.\n",
    "    :param l2_reg_dnn: float . L2 regularizer strength applied to DNN\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param biout_dropout: When not ``None``, the probability we will drop out the output of BiInteractionPooling Layer.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in deep net\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "    :return: A PyTorch model instance.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(128, 128),\n",
    "                 l2_reg_embedding=1e-5, l2_reg_linear=1e-5, l2_reg_dnn=0, init_std=0.0001, seed=1024, bi_dropout=0,\n",
    "                 dnn_dropout=0, dnn_activation='relu', task='binary', device='cpu'):\n",
    "        super(NFM, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                  l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                  device=device)\n",
    "\n",
    "        self.dnn = DNN(self.compute_input_dim(dnn_feature_columns, include_sparse=False) + self.embedding_size,\n",
    "                       dnn_hidden_units,\n",
    "                       activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=False,\n",
    "                       init_std=init_std, device=device)\n",
    "        self.dnn_linear = nn.Linear(\n",
    "            dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "        self.add_regularization_weight(\n",
    "            filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2=l2_reg_dnn)\n",
    "        self.add_regularization_weight(self.dnn_linear.weight, l2=l2_reg_dnn)\n",
    "        self.bi_pooling = BiInteractionPooling()\n",
    "        self.bi_dropout = bi_dropout\n",
    "        if self.bi_dropout > 0:\n",
    "            self.dropout = nn.Dropout(bi_dropout)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "        linear_logit = self.linear_model(X)\n",
    "        fm_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "        bi_out = self.bi_pooling(fm_input)\n",
    "        if self.bi_dropout:\n",
    "            bi_out = self.dropout(bi_out)\n",
    "\n",
    "        dnn_input = combined_dnn_input([bi_out], dense_value_list)\n",
    "        dnn_output = self.dnn(dnn_input)\n",
    "        dnn_logit = self.dnn_linear(dnn_output)\n",
    "\n",
    "        logit = linear_logit + dnn_logit\n",
    "\n",
    "        y_pred = self.out(logit)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "class AFM(BaseModel):\n",
    "    \"\"\"Instantiates the Attentional Factorization Machine architecture.\n",
    "\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param use_attention: bool,whether use attention or not,if set to ``False``.it is the same as **standard Factorization Machine**\n",
    "    :param attention_factor: positive integer,units in attention net\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to linear part\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_att: float. L2 regularizer strength applied to attention net\n",
    "    :param afm_dropout: float in [0,1), Fraction of the attention net output units to dropout.\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "    :return: A PyTorch model instance.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, linear_feature_columns, dnn_feature_columns, use_attention=True, attention_factor=8,\n",
    "                 l2_reg_linear=1e-5, l2_reg_embedding=1e-5, l2_reg_att=1e-5, afm_dropout=0, init_std=0.0001, seed=1024,\n",
    "                 task='binary', device='cpu'):\n",
    "        super(AFM, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                  l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                  device=device)\n",
    "\n",
    "        self.use_attention = use_attention\n",
    "\n",
    "        if use_attention:\n",
    "            self.fm = AFMLayer(self.embedding_size, attention_factor, l2_reg_att, afm_dropout,\n",
    "                               seed, device)\n",
    "            \n",
    "            # 防止过拟合\n",
    "            self.add_regularization_weight(self.fm.attention_W, l2_reg_att)\n",
    "        else:\n",
    "            self.fm = FM()\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        sparse_embedding_list, _ = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                   self.embedding_dict, support_dense=True)\n",
    "        # 一阶项\n",
    "        logit = self.linear_model(X)\n",
    "        \n",
    "        # 二阶注意力交叉项\n",
    "        if len(sparse_embedding_list) > 0:\n",
    "            if self.use_attention:\n",
    "                logit += self.fm(sparse_embedding_list)\n",
    "            else:\n",
    "                logit += self.fm(torch.cat(sparse_embedding_list, dim=1))\n",
    "　　　　　\n",
    "        # 预测\n",
    "        y_pred = self.out(logit)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFMLayer\n",
    "![](./imgs/att.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFMLayer(nn.Module):\n",
    "    \"\"\"Attentonal Factorization Machine models pairwise (order-2) feature\n",
    "    interactions without linear term and bias.\n",
    "      Input shape\n",
    "        - A list of 3D tensor with shape: ``(batch_size,1,embedding_size)``.\n",
    "      Output shape\n",
    "        - 2D tensor with shape: ``(batch_size, 1)``.\n",
    "      Arguments\n",
    "        - **in_features** : Positive integer, dimensionality of input features.\n",
    "        - **attention_factor** : Positive integer, dimensionality of the\n",
    "         attention network output space.\n",
    "        - **l2_reg_w** : float between 0 and 1. L2 regularizer strength\n",
    "         applied to attention network.\n",
    "        - **dropout_rate** : float between in [0,1). Fraction of the attention net output units to dropout.\n",
    "        - **seed** : A Python integer to use as random seed.\n",
    "      References\n",
    "        - [Attentional Factorization Machines : Learning the Weight of Feature\n",
    "        Interactions via Attention Networks](https://arxiv.org/pdf/1708.04617.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, attention_factor=4, l2_reg_w=0, dropout_rate=0, seed=1024, device='cpu'):\n",
    "        super(AFMLayer, self).__init__()\n",
    "        self.attention_factor = attention_factor\n",
    "        self.l2_reg_w = l2_reg_w\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.seed = seed\n",
    "        embedding_size = in_features\n",
    "\n",
    "        self.attention_W = nn.Parameter(torch.Tensor(\n",
    "            embedding_size, self.attention_factor))\n",
    "\n",
    "        self.attention_b = nn.Parameter(torch.Tensor(self.attention_factor))\n",
    "\n",
    "        self.projection_h = nn.Parameter(\n",
    "            torch.Tensor(self.attention_factor, 1))\n",
    "\n",
    "        self.projection_p = nn.Parameter(torch.Tensor(embedding_size, 1))\n",
    "\n",
    "        for tensor in [self.attention_W, self.projection_h, self.projection_p]:\n",
    "            nn.init.xavier_normal_(tensor, )\n",
    "\n",
    "        for tensor in [self.attention_b]:\n",
    "            nn.init.zeros_(tensor, )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds_vec_list = inputs\n",
    "        row = []\n",
    "        col = []\n",
    "\n",
    "        # 组合所有可能的二阶特征交叉\n",
    "        for r, c in itertools.combinations(embeds_vec_list, 2):\n",
    "            row.append(r)\n",
    "            col.append(c)\n",
    "\n",
    "        # 获得交叉项的左半部分 x_i\n",
    "        # (B, 交差项数目, E)\n",
    "        p = torch.cat(row, dim=1)\n",
    "        \n",
    "        # 获得交叉项的有半部分 x_j\n",
    "        q = torch.cat(col, dim=1)\n",
    "        \n",
    "        # 计算交叉项目 x_i * x_j\n",
    "        inner_product = p * q\n",
    "\n",
    "        # 输入交叉项目\n",
    "        # (B, 交叉项目数， E)\n",
    "        bi_interaction = inner_product\n",
    "        \n",
    "        # 单层全连接\n",
    "        # (B, 交叉项目数， attention_factor)\n",
    "        attention_temp = F.relu(torch.tensordot(\n",
    "            bi_interaction, self.attention_W, dims=([-1], [0])) + self.attention_b)\n",
    "\n",
    "        # 计算每一个交叉项目的注意力权重\n",
    "        # (B, 交差项目数， 1)\n",
    "        self.normalized_att_score = F.softmax(torch.tensordot(\n",
    "            attention_temp, self.projection_h, dims=([-1], [0])), dim=1)\n",
    "        \n",
    "        # 所有二阶特征加权求和(B, E)\n",
    "        attention_output = torch.sum(\n",
    "            self.normalized_att_score * bi_interaction, dim=1)\n",
    "        \n",
    "        # 防止过拟合\n",
    "        attention_output = self.dropout(attention_output)  # training\n",
    "\n",
    "        # (B, 1)\n",
    "        afm_out = torch.tennsordot(\n",
    "            attention_output, self.projection_p, dims=([-1], [0]))\n",
    "\n",
    "        return afm_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[论文地址](https://arxiv.org/abs/1708.04617)  \n",
    "[AFM](https://zhuanlan.zhihu.com/p/94009156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 知识点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2], [2, 5])\n",
      "([1, 2], [4, 5])\n",
      "([2, 5], [4, 5])\n"
     ]
    }
   ],
   "source": [
    "# 能够实现特征的两两组合\n",
    "import itertools\n",
    "A = list(([1, 2], [2, 5], [4, 5]))\n",
    "for com in itertools.combinations(A, 2):\n",
    "    print(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_embedding_list\n",
    "C = [torch.rand((32, 1, 4)) for i in range(26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 325, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "row = []\n",
    "col = []\n",
    "\n",
    "for r, c in itertools.combinations(C, 2):\n",
    "    row.append(r)\n",
    "    col.append(c)\n",
    "\n",
    "x_i = torch.cat(row, dim=1)\n",
    "x_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 特征两两组合的所有可能数目\n",
    "13*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
