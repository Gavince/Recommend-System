{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "SEED = 42\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    \"\"\"\n",
    "    From keras sorucecode: https://github.com/keras-team/keras/blob/master/keras/utils/np_utils.py#L9\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "def data_preparation():\n",
    "    # The column names are from\n",
    "    # https://www2.1010data.com/documentationcenter/prod/Tutorials/MachineLearningExamples/CensusIncomeDataSet.html\n",
    "    column_names = ['age', 'class_worker', 'det_ind_code', 'det_occ_code', 'education', 'wage_per_hour', 'hs_college',\n",
    "                    'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member',\n",
    "                    'unemp_reason', 'full_or_part_emp', 'capital_gains', 'capital_losses', 'stock_dividends',\n",
    "                    'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ',\n",
    "                    'instance_weight', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',\n",
    "                    'num_emp', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',\n",
    "                    'own_or_self', 'vet_question', 'vet_benefits', 'weeks_worked', 'year', 'income_50k']\n",
    "\n",
    "    # Load the dataset in Pandas\n",
    "    train_df = pd.read_csv(\n",
    "        'data/census-income.data.gz',\n",
    "        delimiter=',',\n",
    "        header=None,\n",
    "        index_col=None,\n",
    "        names=column_names\n",
    "    )\n",
    "    other_df = pd.read_csv(\n",
    "        'data/census-income.test.gz',\n",
    "        delimiter=',',\n",
    "        header=None,\n",
    "        index_col=None,\n",
    "        names=column_names\n",
    "    )\n",
    "\n",
    "    # First group of tasks according to the paper\n",
    "    label_columns = ['income_50k', 'marital_stat']\n",
    "\n",
    "    # One-hot encoding categorical columns\n",
    "    categorical_columns = ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'major_ind_code',\n",
    "                           'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason',\n",
    "                           'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat',\n",
    "                           'det_hh_summ', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',\n",
    "                           'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',\n",
    "                           'vet_question']\n",
    "    train_raw_labels = train_df[label_columns]\n",
    "    other_raw_labels = other_df[label_columns]\n",
    "    transformed_train = pd.get_dummies(train_df.drop(label_columns, axis=1), columns=categorical_columns)\n",
    "    transformed_other = pd.get_dummies(other_df.drop(label_columns, axis=1), columns=categorical_columns)\n",
    "\n",
    "    # Filling the missing column in the other set\n",
    "    transformed_other['det_hh_fam_stat_ Grandchild <18 ever marr not in subfamily'] = 0\n",
    "\n",
    "    # One-hot encoding categorical labels\n",
    "    train_income = to_categorical((train_raw_labels.income_50k == ' 50000+.').astype(int), num_classes=2)\n",
    "    train_marital = to_categorical((train_raw_labels.marital_stat == ' Never married').astype(int), num_classes=2)\n",
    "    other_income = to_categorical((other_raw_labels.income_50k == ' 50000+.').astype(int), num_classes=2)\n",
    "    other_marital = to_categorical((other_raw_labels.marital_stat == ' Never married').astype(int), num_classes=2)\n",
    "\n",
    "    dict_outputs = {\n",
    "        'income': train_income.shape[1],\n",
    "        'marital': train_marital.shape[1]\n",
    "    }\n",
    "    dict_train_labels = {\n",
    "        'income': train_income,\n",
    "        'marital': train_marital\n",
    "    }\n",
    "    dict_other_labels = {\n",
    "        'income': other_income,\n",
    "        'marital': other_marital\n",
    "    }\n",
    "    output_info = [(dict_outputs[key], key) for key in sorted(dict_outputs.keys())]\n",
    "\n",
    "    # Split the other dataset into 1:1 validation to test according to the paper\n",
    "    validation_indices = transformed_other.sample(frac=0.5, replace=False, random_state=SEED).index\n",
    "    test_indices = list(set(transformed_other.index) - set(validation_indices))\n",
    "    validation_data = transformed_other.iloc[validation_indices]\n",
    "    validation_label = [dict_other_labels[key][validation_indices] for key in sorted(dict_other_labels.keys())]\n",
    "    test_data = transformed_other.iloc[test_indices]\n",
    "    test_label = [dict_other_labels[key][test_indices] for key in sorted(dict_other_labels.keys())]\n",
    "    train_data = transformed_train\n",
    "    train_label = [dict_train_labels[key] for key in sorted(dict_train_labels.keys())]\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label, output_info\n",
    "\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label, output_info = data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTensorDataset(my_x, my_y):\n",
    "    tensor_x = torch.Tensor(my_x)\n",
    "    tensor_y = torch.Tensor(my_y)\n",
    "    return torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "train_label_tmp = np.column_stack((np.argmax(train_label[0], axis=1), np.argmax(train_label[1], axis=1)))\n",
    "train_loader = DataLoader(dataset=getTensorDataset(train_data.to_numpy(), train_label_tmp), batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_label_tmp = np.column_stack((np.argmax(validation_label[0], axis=1), np.argmax(validation_label[1], axis=1)))\n",
    "val_loader = DataLoader(dataset=getTensorDataset(validation_data.to_numpy(), validation_label_tmp), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # self.log_soft = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.log_soft(out)\n",
    "        return out\n",
    "    \n",
    "class Tower(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Tower, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.softmax(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMOE(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_experts, experts_out, experts_hidden, towers_hidden, tasks):\n",
    "        super(MMOE, self).__init__()\n",
    "        # params\n",
    "        self.input_size = input_size\n",
    "        self.num_experts = num_experts\n",
    "        self.experts_out = experts_out\n",
    "        self.experts_hidden = experts_hidden\n",
    "        self.towers_hidden = towers_hidden\n",
    "        self.tasks = tasks\n",
    "        # row by row\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # model\n",
    "        self.experts = nn.ModuleList([Expert(self.input_size, self.experts_out, self.experts_hidden) for i in range(self.num_experts)])\n",
    "        self.w_gates = nn.ParameterList([nn.Parameter(torch.randn(input_size, num_experts), requires_grad=True) for i in range(self.tasks)])\n",
    "        self.towers = nn.ModuleList([Tower(self.experts_out, 1, self.towers_hidden) for i in range(self.tasks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the experts output\n",
    "        expers_o = [e(x) for e in self.experts]\n",
    "        expers_o_tensor = torch.stack(expers_o)\n",
    "        \n",
    "        # get the gates output\n",
    "        gates_o = [self.softmax(x @ g) for g in self.w_gates]\n",
    "        \n",
    "        # multiply the output of the experts with the corresponding gates output\n",
    "        # res = gates_o[0].t().unsqueeze(2).expand(-1, -1, self.experts_out) * expers_o_tensor\n",
    "        # https://discuss.pytorch.org/t/element-wise-multiplication-of-the-last-dimension/79534\n",
    "        towers_input = [g.t().unsqueeze(2).expand(-1, -1, self.experts_out) * expers_o_tensor for g in gates_o]\n",
    "        towers_input = [torch.sum(ti, dim=0) for ti in towers_input]\n",
    "        \n",
    "        # get the final output from the towers\n",
    "        final_output = [t(ti) for t, ti in zip(self.towers, towers_input)]\n",
    "        \n",
    "        # get the output of the towers, and stack them\n",
    "        # final_output = torch.stack(final_output, dim=1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_input = torch.tensor([[10.0, 10.0, 15.0, 30.0, 18.0], [20.0, 50.0, 28.0, 22.0, 12.0], [20.0, 50.0, 28.0, 22.0, 12.0]])\n",
    "mmoe = MMOE(input_size=5, num_experts=3, experts_out=4, experts_hidden=2, towers_hidden=2, tasks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2640],\n",
       "         [0.5260],\n",
       "         [0.5105]], grad_fn=<SigmoidBackward>),\n",
       " tensor([[0.3809],\n",
       "         [0.3776],\n",
       "         [0.3809]], grad_fn=<SigmoidBackward>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmoe(simple_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MMOE(input_size=499, num_experts=6, experts_out=16, experts_hidden=32, towers_hidden=8, tasks=2)\n",
    "model = model.to(device)\n",
    "# print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 1/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 2/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 3/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 4/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 5/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 6/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 7/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 8/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 9/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 10/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 11/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 12/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 13/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 14/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 15/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 16/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 17/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 18/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 19/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 20/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 21/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 22/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 23/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 24/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 25/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 26/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 27/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 28/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 29/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 30/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 31/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 32/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 33/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 34/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 35/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 36/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 37/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 38/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 39/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 40/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 41/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 42/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 43/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 44/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 45/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 46/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 47/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 48/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 49/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 50/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 51/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 52/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 53/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 54/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 55/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 56/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 57/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 58/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 59/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 60/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 61/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 62/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 63/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 64/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 65/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 66/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 67/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 68/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 69/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 70/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 71/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 72/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 73/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 74/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 75/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 76/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 77/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 78/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 79/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 80/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 81/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 82/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 83/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 84/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 85/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 86/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 87/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 88/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 89/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 90/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 91/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 92/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 93/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 94/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 95/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 96/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 97/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 98/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 99/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "0.37767263672061463\n",
      "0.4625617153729712\n"
     ]
    }
   ],
   "source": [
    "# Sets hyper-parameters\n",
    "lr = 1e-4\n",
    "n_epochs = 100\n",
    "\n",
    "# Defines loss function and optimizer\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "loss_fn = nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # Uses loader to fetch one mini-batch for training\n",
    "    epoch_loss = []\n",
    "    c = 0\n",
    "    print(\"Epoch: {}/{}\".format(epoch, n_epochs))\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # NOW, sends the mini-batch data to the device\n",
    "        # so it matches location of the MODEL\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        # One stpe of training\n",
    "        yhat = model(x_batch)\n",
    "        \n",
    "        # loss = loss_fn(yhat, y_batch)\n",
    "        \n",
    "        y_batch_t1, y_batch_t2 = y_batch[:, 0], y_batch[:, 1]\n",
    "        yhat_t1, yhat_t2 = yhat[0], yhat[1]\n",
    "        \n",
    "        loss_t1 = loss_fn(yhat_t1, y_batch_t1.view(-1, 1))\n",
    "        loss_t2 = loss_fn(yhat_t2, y_batch_t2.view(-1, 1))\n",
    "        loss = loss_t1 + loss_t2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        if c % 50 == 0:\n",
    "            print(\"    Batch: {}/{}\".format(c, int(len(train_data)/BATCH_SIZE)))\n",
    "        c += 1\n",
    "    losses.append(np.mean(epoch_loss))\n",
    "        \n",
    "    # After finishing training steps for all mini-batches,\n",
    "    # it is time for evaluation!\n",
    "        \n",
    "    # We tell PyTorch to NOT use autograd...\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for validation\n",
    "        epoch_loss = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_val)\n",
    "            # Computes validation loss\n",
    "            # val_loss = loss_fn(yhat, y_val)\n",
    "            \n",
    "            y_val_t1, y_val_t2 = y_val[:, 0], y_val[:, 1]\n",
    "            yhat_t1, yhat_t2 = yhat[0], yhat[1]\n",
    "\n",
    "            loss_t1 = loss_fn(yhat_t1, y_val_t1.view(-1, 1))\n",
    "            loss_t2 = loss_fn(yhat_t2, y_val_t2.view(-1, 1))\n",
    "            loss = loss_t1 + loss_t2\n",
    "            \n",
    "            epoch_loss.append(loss.item())\n",
    "    val_losses.append(np.mean(epoch_loss))\n",
    "\n",
    "# print(model.state_dict())\n",
    "print(np.mean(losses))\n",
    "print(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, n_epochs+1)\n",
    "plt.plot(epochs, losses, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_tmp = np.column_stack((np.argmax(test_label[0], axis=1), np.argmax(test_label[1], axis=1)))\n",
    "test_loader = DataLoader(dataset=getTensorDataset(test_data.to_numpy(), test_label_tmp), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5528050381310132\n"
     ]
    }
   ],
   "source": [
    "t1_pred = []\n",
    "t2_pred = []\n",
    "t1_target = []\n",
    "t2_target = []\n",
    "\n",
    "# We tell PyTorch to NOT use autograd...\n",
    "with torch.no_grad():\n",
    "    # Uses loader to fetch one mini-batch for testing\n",
    "    epoch_loss = []\n",
    "    for x_test, y_test in test_loader:\n",
    "        # Again, sends data to same device as model\n",
    "        x_test = x_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        # Makes predictions\n",
    "        yhat = model(x_test)\n",
    "\n",
    "        y_test_t1, y_test_t2 = y_test[:, 0], y_test[:, 1]\n",
    "        yhat_t1, yhat_t2 = yhat[0], yhat[1]\n",
    "\n",
    "        loss_t1 = loss_fn(yhat_t1, y_test_t1.view(-1, 1))\n",
    "        loss_t2 = loss_fn(yhat_t2, y_test_t2.view(-1, 1))\n",
    "        loss = loss_t1 + loss_t2\n",
    "        \n",
    "        # predict\n",
    "        t1_hat = yhat_t1.view(-1) > 0.5\n",
    "        t2_hat = yhat_t2.view(-1) > 0.5\n",
    "        \n",
    "        # save\n",
    "        t1_pred.append(t1_hat)\n",
    "        t2_pred.append(t2_hat)\n",
    "        t1_target.append(y_test_t1)\n",
    "        t2_target.append(y_test_t2)\n",
    "        \n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "print(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_pred = torch.cat(t1_pred)\n",
    "t2_pred = torch.cat(t2_pred)\n",
    "t1_target = torch.cat(t1_target)\n",
    "t2_target = torch.cat(t2_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46681,   116],\n",
       "       [ 2371,   713]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(t1_target.cpu().numpy(), t1_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97     46797\n",
      "         1.0       0.86      0.23      0.36      3084\n",
      "\n",
      "    accuracy                           0.95     49881\n",
      "   macro avg       0.91      0.61      0.67     49881\n",
      "weighted avg       0.95      0.95      0.94     49881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t1_target.cpu().numpy(), t1_pred.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.87      0.89     28284\n",
      "         1.0       0.84      0.90      0.87     21597\n",
      "\n",
      "    accuracy                           0.88     49881\n",
      "   macro avg       0.88      0.88      0.88     49881\n",
      "weighted avg       0.88      0.88      0.88     49881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t2_target.cpu().numpy(), t2_pred.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing example for the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5821)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([[[0.1, 0.5], [0.1, 0.5]], [[0.1, 0.5], [0.3, 0.5]], [[0.1, 0.5], [1.0, 1.0]]])\n",
    "target = torch.tensor([[0, 0], [1, 1], [1, 1]])\n",
    "output = loss(input, target)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
