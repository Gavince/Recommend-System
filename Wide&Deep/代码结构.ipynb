{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wide&Deep模型代码探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Author:\n",
    "    Weichen Shen,wcshen1994@163.com\n",
    "Reference:\n",
    "    [1] Cheng H T, Koc L, Harmsen J, et al. Wide & deep learning for recommender systems[C]//Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 2016: 7-10.(https://arxiv.org/pdf/1606.07792.pdf)\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from .basemodel import BaseModel\n",
    "from ..inputs import combined_dnn_input\n",
    "from ..layers import DNN\n",
    "\n",
    "\n",
    "class WDL(BaseModel):\n",
    "    \"\"\"Instantiates the Wide&Deep Learning architecture.\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to wide part\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "    :return: A PyTorch model instance.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128),\n",
    "                 l2_reg_linear=1e-5,\n",
    "                 l2_reg_embedding=1e-5, l2_reg_dnn=0, init_std=0.0001, seed=1024, dnn_dropout=0, dnn_activation='relu',\n",
    "                 dnn_use_bn=False,\n",
    "                 task='binary', device='cpu'):\n",
    "\n",
    "        super(WDL, self).__init__(linear_feature_columns, dnn_feature_columns, l2_reg_linear=l2_reg_linear,\n",
    "                                  l2_reg_embedding=l2_reg_embedding, init_std=init_std, seed=seed, task=task,\n",
    "                                  device=device)\n",
    "\n",
    "        self.use_dnn = len(dnn_feature_columns) > 0 and len(\n",
    "            dnn_hidden_units) > 0\n",
    "        if self.use_dnn:\n",
    "            self.dnn = DNN(self.compute_input_dim(dnn_feature_columns), dnn_hidden_units,\n",
    "                           activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                           init_std=init_std, device=device)\n",
    "            self.dnn_linear = nn.Linear(dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "            self.add_regularization_weight(\n",
    "                filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2_reg_dnn)\n",
    "            self.add_regularization_weight(self.dnn_linear.weight, l2_reg_dnn)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                                  self.embedding_dict)\n",
    "        logit = self.linear_model(X)\n",
    "\n",
    "        if self.use_dnn:\n",
    "            dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "\n",
    "            dnn_output = self.dnn(dnn_input)\n",
    "            dnn_logit = self.dnn_linear(dnn_output)\n",
    "            logit += dnn_logit\n",
    "\n",
    "        y_pred = self.out(logit)\n",
    "\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
