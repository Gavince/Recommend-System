{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多路召回\n",
    "<img src=\"./imgs/17393f3dbdbe7a90.png\" style=\"zoom:50%\" />   \n",
    "　　所谓的“多路召回策略”就是指采用不同的策略、特征或者简单模型，分别召回一部分候选集，然后再把这些候选集混合在一起后供后续排序模型使用的策略。  \n",
    "然后我们来说说为啥需要用到多路召回策略，我们在设计召回层的时候，“计算速度”与“召回率”这两个指标是相互矛盾的，也就是说在提高计算速度的时候需要尽量简化召回策略，这就会导致召回率不尽人意，同样的，需要提高召回率时就需要复杂的召回策略，这样计算速度肯定会相应的降低。在权衡两者后，目前工业界普遍采用多个简单的召回策略叠加的“多路召回策略”。   \n",
    "　　在多路召回中，每个策略之间毫不相关，所以一般可以写并发多线程同时进行。例如：新闻类的推荐系统中，我们可以按文章类别、作者、热度等分别进行召回，这样召回出来的结果更贴切实际要求，同时我们可以开辟多个线程分别进行这些召回策略，这样可以更加高效。具体的如下:  \n",
    "<img src=\"./imgs/17393f42d7e616a3.png\" style=\"zoom:45%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oem/anaconda3/envs/zwynn/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from tqdm import tqdm  \n",
    "from collections import defaultdict  \n",
    "import os, math, warnings, math, pickle\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import collections\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "save_path = './data/'\n",
    "# 做召回评估的一个标志, 如果不进行评估就是直接使用全量数据进行召回\n",
    "metric_recall = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug模式： 从训练集中划出一部分数据来调试代码\n",
    "def get_all_click_sample(data_path, sample_nums=10000):\n",
    "    \"\"\"\n",
    "        训练集中采样一部分数据调试\n",
    "        data_path: 原数据的存储路径\n",
    "        sample_nums: 采样数目（这里由于机器的内存限制，可以采样用户做）\n",
    "    \"\"\"\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    all_user_ids = all_click.user_id.unique()\n",
    "\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False) \n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取点击数据，这里分成线上和线下，如果是为了获取线上提交结果应该讲测试集中的点击数据合并到总的数据中\n",
    "# 如果是为了线下验证模型的有效性或者特征的有效性，可以只使用训练集\n",
    "def get_all_click_df(data_path='./data/', offline=True):\n",
    "    if offline:\n",
    "        all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    else:\n",
    "        trn_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "        tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "\n",
    "        all_click = trn_click.append(tst_click)\n",
    "    \n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文章的基本属性\n",
    "def get_item_info_df(data_path):\n",
    "    item_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    \n",
    "    # 为了方便与训练集中的click_article_id拼接，需要把article_id修改成click_article_id\n",
    "    item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n",
    "    \n",
    "    return item_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文章的Embedding数据\n",
    "def get_item_emb_dict(data_path):\n",
    "    \n",
    "    item_emb_df = pd.read_csv(data_path + 'articles_emb.csv')\n",
    "    \n",
    "    # 取出所有的emb向量\n",
    "    item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])\n",
    "    # 进行归一化\n",
    "    item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "\n",
    "    item_emb_dict = dict(zip(item_emb_df['article_id'], item_emb_np))\n",
    "    pickle.dump(item_emb_dict, open(\"./data/\" + 'item_content_emb.pkl', 'wb'))\n",
    "    \n",
    "    return item_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199999</td>\n",
       "      <td>160417</td>\n",
       "      <td>1507029570190</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199999</td>\n",
       "      <td>5408</td>\n",
       "      <td>1507029571478</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199999</td>\n",
       "      <td>50823</td>\n",
       "      <td>1507029601478</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199998</td>\n",
       "      <td>157770</td>\n",
       "      <td>1507029532200</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>199998</td>\n",
       "      <td>96613</td>\n",
       "      <td>1507029671831</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  click_article_id  click_timestamp  click_environment  \\\n",
       "0   199999            160417    1507029570190                  4   \n",
       "1   199999              5408    1507029571478                  4   \n",
       "2   199999             50823    1507029601478                  4   \n",
       "3   199998            157770    1507029532200                  4   \n",
       "4   199998             96613    1507029671831                  4   \n",
       "\n",
       "   click_deviceGroup  click_os  click_country  click_region  \\\n",
       "0                  1        17              1            13   \n",
       "1                  1        17              1            13   \n",
       "2                  1        17              1            13   \n",
       "3                  1        17              1            25   \n",
       "4                  1        17              1            25   \n",
       "\n",
       "   click_referrer_type  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    5  \n",
       "4                    5  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_click_df = get_all_click_df(offline=False)\n",
    "all_click_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_click_df['click_timestamp'] = all_click_df[['click_timestamp']].apply(max_min_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>created_at_ts</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1513144419000</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1405341936000</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1408667706000</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1408468313000</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1407071171000</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click_article_id  category_id  created_at_ts  words_count\n",
       "0                 0            0  1513144419000          168\n",
       "1                 1            1  1405341936000          189\n",
       "2                 2            1  1408667706000          250\n",
       "3                 3            1  1408468313000          230\n",
       "4                 4            1  1407071171000          162"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_info_df = get_item_info_df(\"./data/\")\n",
    "item_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb_dict = get_item_emb_dict(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得用户-文章-时间函数\n",
    "计算用户相似度是需要用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_item_time(click_df):\n",
    "    \"\"\"\n",
    "    {user:[(item, time)......]}\n",
    "    \"\"\"\n",
    "    \n",
    "    click_df = click_df.sort_values(by=\"click_timestamp\")\n",
    "    \n",
    "    def make_item_time_pair(df):\n",
    "        \n",
    "        return list(zip(df[\"click_article_id\"], df[\"click_timestamp\"]))\n",
    "    \n",
    "    user_item_time_df = click_df.groupby(\"user_id\")[\"click_article_id\", \n",
    "                                                     \"click_timestamp\"].apply(lambda x: make_item_time_pair(x)).reset_index().rename(columns={0:\"item_time_list\"})\n",
    "    \n",
    "    user_tem_time_dict = dict(zip(user_item_time_df[\"user_id\"], user_item_time_df[\"item_time_list\"]))\n",
    "    \n",
    "    return user_tem_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获得文章-用户-时间函数\n",
    "计算物品相似度是需要用到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_user_time_dict(click_df):\n",
    "    \"\"\"\n",
    "    {\"item\":[(user, time)]......}\n",
    "    \"\"\"\n",
    "    \n",
    "    def make_user_time_pair(df):\n",
    "        \n",
    "        return list(zip(df['user_id'], df['click_timestamp']))\n",
    "    \n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    item_user_time_df = click_df.groupby('click_article_id')['user_id', 'click_timestamp'].apply(lambda x: make_user_time_pair(x))\\\n",
    "                                                            .reset_index().rename(columns={0: 'user_time_list'})\n",
    "    \n",
    "    item_user_time_dict = dict(zip(item_user_time_df['click_article_id'], item_user_time_df['user_time_list']))\n",
    "    \n",
    "    return item_user_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取用户历史和最后一次点击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意：[:-1] 左闭右开\n",
    "\n",
    "def get_hist_and_get_last_click(all_click):\n",
    "    \"\"\"计算点击\"\"\"\n",
    "    \n",
    "    all_click = all_click.sort_values(by=[\"user_id\", \"click_timestamp\"])\n",
    "    click_last_df = all_click.groupby(\"user_id\").tail(1)\n",
    "    \n",
    "    def hist_func(df):\n",
    "        \n",
    "        if len(df) == 1:\n",
    "            \"\"\"只有一个用户\"\"\"\n",
    "            return df\n",
    "        else:\n",
    "            return df[:-1]  # 用户历史点记录, 不包括最后一次点击\n",
    "    \n",
    "    click_hist_df = all_click.groupby(\"user_id\").apply(hist_func).reset_index(drop=True) \n",
    "    \n",
    "    return click_hist_df, click_last_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取新闻id对应的属性值\n",
    "def get_item_info_dict(item_info_df):\n",
    "    \n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    item_info_df['created_at_ts'] = item_info_df[['created_at_ts']].apply(max_min_scaler)\n",
    "    \n",
    "    item_type_dict = dict(zip(item_info_df['click_article_id'], item_info_df['category_id']))\n",
    "    item_words_dict = dict(zip(item_info_df['click_article_id'], item_info_df['words_count']))\n",
    "    item_created_time_dict = dict(zip(item_info_df['click_article_id'], item_info_df['created_at_ts']))\n",
    "    \n",
    "    return item_type_dict, item_words_dict, item_created_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取用户相对应的属性值\n",
    "def get_user_hist_item_info_dict(all_click):\n",
    "    \n",
    "    # 获取user_id对应的用户历史点击文章类型的集合字典\n",
    "    user_hist_item_typs = all_click.groupby('user_id')['category_id'].agg(set).reset_index()\n",
    "    user_hist_item_typs_dict = dict(zip(user_hist_item_typs['user_id'], user_hist_item_typs['category_id']))\n",
    "    \n",
    "    # 获取user_id对应的用户点击文章的集合\n",
    "    user_hist_item_ids_dict = all_click.groupby('user_id')['click_article_id'].agg(set).reset_index()\n",
    "    user_hist_item_ids_dict = dict(zip(user_hist_item_ids_dict['user_id'], user_hist_item_ids_dict['click_article_id']))\n",
    "    \n",
    "    # 获取user_id对应的用户历史点击的文章的平均字数字典\n",
    "    user_hist_item_words = all_click.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "    user_hist_item_words_dict = dict(zip(user_hist_item_words['user_id'], user_hist_item_words['words_count']))\n",
    "    \n",
    "    # 获取user_id对应的用户最后一次点击的文章的创建时间\n",
    "    all_click_ = all_click.sort_values('click_timestamp')\n",
    "    user_last_item_created_time = all_click_.groupby('user_id')['created_at_ts'].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "    \n",
    "    max_min_scaler = lambda x : (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    user_last_item_created_time['created_at_ts'] = user_last_item_created_time[['created_at_ts']].apply(max_min_scaler)\n",
    "    \n",
    "    user_last_item_created_time_dict = dict(zip(user_last_item_created_time['user_id'], \\\n",
    "                                                user_last_item_created_time['created_at_ts']))\n",
    "    \n",
    "    \n",
    "    return user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取点击次数做多的topk个文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_topk_click(clicl_df, k):\n",
    "    \"\"\"Topk 元素\"\"\"\n",
    "    \n",
    "    topk_click = clicl_df[\"click_article_id\"].value_counts().index[:k]\n",
    "    \n",
    "    return topk_click\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取每一篇文章的具体属性信息，凑成字典，诸如类型\n",
    "item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>created_at_ts</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.978432</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.680295</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.689493</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688942</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.685078</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click_article_id  category_id  created_at_ts  words_count\n",
       "0                 0            0       0.978432          168\n",
       "1                 1            1       0.680295          189\n",
       "2                 2            1       0.689493          250\n",
       "3                 3            1       0.688942          230\n",
       "4                 4            1       0.685078          162"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义多路召回的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个多路召回的字典，将各路召回的结果都保存在这个字典当中\n",
    "user_multi_recall_dict =  {'itemcf_sim_itemcf_recall': {},\n",
    "                           'embedding_sim_item_recall': {},\n",
    "                           'youtubednn_recall': {},\n",
    "                           'youtubednn_usercf_recall': {}, \n",
    "                           'cold_start_recall': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_hist_click_df, trn_last_click_df = get_hist_and_get_last_click(all_click_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 召回效果评估函数  \n",
    "做完了召回有时候也需要对当前的召回方法或者参数进行调整以达到更好的召回效果，因为召回的结果决定了最终排序的上限，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metics_recall(user_recall_items_dict, trn_last_clicl_df, topk=5):\n",
    "    \"\"\"依次召回前10, 20, 30, 40, 50个文章的击中率\"\"\"\n",
    "    \n",
    "    # 用户最后点击的的文章\n",
    "    last_click_item_dict = dict(zip(trn_last_click_df[\"user_id\"], trn_last_click_df[\"click_article_id\"]))\n",
    "    \n",
    "    user_num = len(user_recall_items_dict)\n",
    "    \n",
    "    for k in range(10, topk+1, 10):\n",
    "        hit_num = 0\n",
    "        \n",
    "        for user, item in user_recall_items_dict.items():\n",
    "            # 召回前k个\n",
    "            tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n",
    "            if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "                hit_num += 1\n",
    "                \n",
    "        # 以总用户的击中率作为指标\n",
    "        hit_rate = round(hit_num *1.0 / user_num, 5)\n",
    "        \n",
    "        print(\"topk:{}, hit_num:{}, hit_rate:{}, user_num:{}\".format(topk, hit_num, hit_rate, user_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算相似度矩阵\n",
    "基于关联规则:  \n",
    "(1)用户点击的时间权重  \n",
    "(2)用户点击的顺序权重  \n",
    "(3)文章创建的时间权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小测试\n",
    "\n",
    "# i2i_sim = {}\n",
    "# i2i_sim.setdefault(1, {})\n",
    "# for x in range(1, 5):\n",
    "#     i2i_sim[1].setdefault(\"2\", 0)\n",
    "#     i2i_sim[1][\"2\"] += x\n",
    "\n",
    "# i2i_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 物品相似度的协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_sim(df, item_created_time_dict):\n",
    "    \"\"\"\n",
    "        文章与文章之间的相似性矩阵计算\n",
    "        :param df: 数据表\n",
    "        :item_created_time_dict:  文章创建时间的字典\n",
    "        return : 文章与文章的相似性矩阵\n",
    "    \"\"\"\n",
    "    \n",
    "    user_item_time_dict = get_user_item_time(df)\n",
    "    \n",
    "    # 计算物品相似度\n",
    "    i2i_sim = {}\n",
    "    item_cnt = defaultdict(int)\n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "        # 在基于商品的协同过滤优化的时候可以考虑时间因素\n",
    "        for loc1, (i, i_click_time) in enumerate(item_time_list):\n",
    "            item_cnt[i] += 1\n",
    "            i2i_sim.setdefault(i, {})\n",
    "            for loc2, (j, j_click_time) in enumerate(item_time_list):\n",
    "                if(i == j):\n",
    "                    continue\n",
    "                    \n",
    "                # 考虑文章的正向顺序点击和反向顺序点击    \n",
    "                loc_alpha = 1.0 if loc2 > loc1 else 0.7\n",
    "                # 位置信息权重，其中的参数可以调节\n",
    "                loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))\n",
    "                # 点击时间权重，其中的参数可以调节\n",
    "                click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))\n",
    "                # 两篇文章创建时间的权重，其中的参数可以调节\n",
    "                created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                i2i_sim[i].setdefault(j, 0)\n",
    "                # 考虑多种因素的权重计算最终的文章之间的相似度\n",
    "                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "                \n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "    \n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "    \n",
    "    return i2i_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [06:39<00:00, 625.56it/s] \n"
     ]
    }
   ],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户相似度的协同过滤\n",
    "<font color=\"red\">**注意**</font>：用户数据量较大，不容易计算出相对应的用户相似度字典,后面使用YoutubeDNN得到的用户Embedding向量进行相似度的计算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_actvivate_degree_dict(all_click_df):\n",
    "    \"\"\"\n",
    "    统计较为活跃的用户\n",
    "    \"\"\"\n",
    "    \n",
    "    all_click_df_ = all_click_df.groupby('user_id')['click_article_id'].count().reset_index()\n",
    "    \n",
    "    # 将统计数据归一化，便于计算\n",
    "    mm = MinMaxScaler()\n",
    "    all_click_df_[\"click_article_id\"] = mm.fit_transform(all_click_df_[[\"click_article_id\"]])\n",
    "    user_activate_degree_dict = dict(zip(all_click_df_[\"user_id\"], all_click_df_[\"click_article_id\"]))\n",
    "    \n",
    "    return user_activate_degree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usercf_sim(all_click_df, user_activate_degree_dict):\n",
    "    \"\"\"\n",
    "    基于用户活跃度(规则)的用户相似度计算\n",
    "    \"\"\"\n",
    "    \n",
    "    item_user_time_dict = get_item_user_time_dict(all_click_df)\n",
    "    \n",
    "    u2u_sim = {}\n",
    "    user_cnt = defaultdict(int)\n",
    "    \n",
    "    for item, user_item_list in tqdm(item_user_time_dict.items()):\n",
    "        for u, click_time in user_item_list:\n",
    "            user_cnt[u] += 1\n",
    "            u2u_sim.setdefault(u, {})\n",
    "            for v, click_time in user_item_list:\n",
    "                u2u_sim[u].setdefault(v, 0)\n",
    "                if u == v:\n",
    "                    continue\n",
    "                    \n",
    "                activate_weight = 100*0.5*(user_activate_degree_dict[u] + user_activate_degree_dict[v])\n",
    "                u2u_sim[u][v] += activate_weight / math.log(len(user_item_list)+1)\n",
    "                \n",
    "    u2u_sim_ = u2u_sim.copy()\n",
    "    for u, related_users in u2u_sim.items():\n",
    "        for v, wuv in related_users.items():\n",
    "            u2u_sim_[u][v] = wuv/math.sqrt(user_cnt[u], user_cnt[v])\n",
    "    \n",
    "    pickle.dump(u2u_sim_, open(\"./data/\" + \"usercf_u2u_sim.pkl\", \"wb\"))\n",
    "    \n",
    "    return u2u_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_activate_degree_dict =  get_user_actvivate_degree_dict(all_click_df)\n",
    "# u2u_sim = usercf_sim(all_click_df, user_activate_degree_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 物品Embedding相似\n",
    "每一篇文章有一个Embedding向量，通过计算不同文章的Embedding相似度，候选出Topk篇文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于文章内容的相似度计算\n",
    "def embediing_sim(click_df, item_emb_df, save_path = \"./data/\", topk = 5):\n",
    "    \"\"\"\n",
    "    通过Embedding计算物品相似度\n",
    "    \"\"\"\n",
    "    \n",
    "    # 索引与文章id映射\n",
    "    item_idx_2_rawid_dict = dict(zip(item_emb_df.index, item_emb_df[\"article_id\"]))\n",
    "    item_emb_cols = [x for x in item_emb_df.columns if \"emb\" in x]\n",
    "    \n",
    "    # 向量单位化\n",
    "    item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols].values, dtype=np.float32)\n",
    "    item_emb_np = item_emb_np/ np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "    \n",
    "    # 使用faiss建立索引，加快相似度的计算\n",
    "    item_index = faiss.IndexFlatIP(item_emb_np.shape[1])\n",
    "    item_index.add(item_emb_np)\n",
    "    sim, idx = item_index.search(item_emb_np, topk)\n",
    "    \n",
    "    item_sim_dict = collections.defaultdict(dict)  # TODO:查阅资料两层字典的结构{{},......}\n",
    "    \n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(item_emb_np)), sim, idx)):\n",
    "        # 真实文章id \n",
    "        target_raw_id = item_idx_2_rawid_dict[target_idx]\n",
    "        # 最相似的物品是其本身,即为第一个元素, topk-1个商品被候选\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "            # 得到相似的文章id\n",
    "            rele_raw_id = item_idx_2_rawid_dict[rele_idx]\n",
    "            item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value  # {target_raw_id:{rele_raw_id:sim_value}}\n",
    "            \n",
    "    \n",
    "    pickle.dump(item_sim_dict, open(save_path + \"emb_i2i_sim.pkl\", \"wb\"))\n",
    "    \n",
    "    return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "364047it [00:15, 22808.31it/s]\n"
     ]
    }
   ],
   "source": [
    "item_emb_df = pd.read_csv(\"./data/articles_emb.csv\")\n",
    "emb_i2i_dict = embediing_sim(all_click_df, item_emb_df, topk=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 召回\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取双塔召回时的训练验证数据\n",
    "# negsample指的是通过滑窗构建样本的时候，负样本的数量\n",
    "def gen_data_set(data, negsample=0):\n",
    "    \n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "    item_ids = data[\"click_article_id\"].unique()\n",
    "    \n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    for reviewID, hist in tqdm(data.groupby(\"user_id\")):\n",
    "        pos_list = hist[\"click_article_id\"].tolist()\n",
    "        \n",
    "        if negsample > 0: # 对每一正样本进行negsample负样本采样\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            neg_list = np.random.choice(candidate_set, size=len(pos_list)*negsample, replace=True)\n",
    "            \n",
    "        # 特例，长度为1时需要考虑\n",
    "        if len(pos_list) == 1:\n",
    "            train_set.append((reviewID, [pos_list[0]], pos_list[0], 1, len(pos_list)))\n",
    "            test_set.append((reviewID, [pos_list[0]], pos_list[0], 1, len(pos_list)))\n",
    "        \n",
    "        # 滑动窗口构造正负样本, 按照时间顺序，将最后一次访问的文章设置标签为１的正样本\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i]\n",
    "            \n",
    "            if i != len(pos_list) - 1:\n",
    "                # 正样本 [user_id, his_item, pos_item, label, len(his_item)]\n",
    "                train_set.append((reviewID, hist[::-1], pos_list[i], 1, len(hist[::-1])))\n",
    "                \n",
    "                for negi in range(negsample):\n",
    "                    # 负样本 [user_id, his_item, neg_item, label, len(his_item)]\n",
    "                    train_set.append((reviewID, hist[::-1], neg_list[i*negsample+negi], 0, len(hist[::-1])))\n",
    "            else:\n",
    "                test_set.append((reviewID, hist[::-1], pos_list[i], 1, len(hist[::-1])))\n",
    "                    \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将输入数据进行padding，使得序列特征的长度都一样\n",
    "# 将输入的数据进行padding，使得序列特征的长度都一致\n",
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    train_model_input = {\"user_id\": train_uid, \"click_article_id\": train_iid, \"hist_article_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YoutubeDNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./imgs/image-20201111160516562.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YoutubeDNN_(user_feature_columns, item_feature_columns, num_sampled=5,\n",
    "               user_dnn_hidden_units=(64, 32),\n",
    "               dnn_activation='relu', dnn_use_bn=False,\n",
    "               l2_reg_dnn=0, l2_reg_embedding=1e-6, dnn_dropout=0, output_activation='linear', seed=1024, ):\n",
    "    \n",
    "    \"\"\"Instantiates the YoutubeDNN Model architecture.\n",
    "    :param user_feature_columns: An iterable containing user's features used by  the model.\n",
    "    :param item_feature_columns: An iterable containing item's features used by  the model.\n",
    "    :param num_sampled: int, the number of classes to randomly sample per batch.\n",
    "    :param user_dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of user tower\n",
    "    :param dnn_activation: Activation function to use in deep net\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in deep net\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param output_activation: Activation function to use in output layer\n",
    "    :return: A Keras model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(item_feature_columns) > 1:\n",
    "        raise ValueError(\"Now YoutubeNN only support 1 item feature like item_id\")\n",
    "    item_feature_name = item_feature_columns[0].name\n",
    "    item_vocabulary_size = item_feature_columns[0].vocabulary_size\n",
    "\n",
    "    embedding_matrix_dict = create_embedding_matrix(user_feature_columns + item_feature_columns, l2_reg_embedding,\n",
    "                                                    seed=seed)\n",
    "\n",
    "    user_features = build_input_features(user_feature_columns)\n",
    "    user_inputs_list = list(user_features.values())\n",
    "    user_sparse_embedding_list, user_dense_value_list = input_from_feature_columns(user_features, user_feature_columns,\n",
    "                                                                                   l2_reg_embedding, seed=seed,\n",
    "                                                                                   embedding_matrix_dict=embedding_matrix_dict)\n",
    "    user_dnn_input = combined_dnn_input(user_sparse_embedding_list, user_dense_value_list)\n",
    "\n",
    "    item_features = build_input_features(item_feature_columns)\n",
    "    item_inputs_list = list(item_features.values())\n",
    "    user_dnn_out = DNN(user_dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout,\n",
    "                       dnn_use_bn, output_activation=output_activation, seed=seed)(user_dnn_input)\n",
    "\n",
    "    item_index = EmbeddingIndex(list(range(item_vocabulary_size)))(item_features[item_feature_name])\n",
    "\n",
    "    item_embedding_matrix = embedding_matrix_dict[\n",
    "        item_feature_name]\n",
    "    item_embedding_weight = NoMask()(item_embedding_matrix(item_index))\n",
    "\n",
    "    pooling_item_embedding_weight = PoolingLayer()([item_embedding_weight])\n",
    "\n",
    "    output = SampledSoftmaxLayer(num_sampled=num_sampled)(\n",
    "        [pooling_item_embedding_weight, user_dnn_out, item_features[item_feature_name]])\n",
    "    model = Model(inputs=user_inputs_list + item_inputs_list, outputs=output)\n",
    "\n",
    "    model.__setattr__(\"user_input\", user_inputs_list)  # 设置新的属性值\n",
    "    model.__setattr__(\"user_embedding\", user_dnn_out) # Embedding\n",
    "\n",
    "    model.__setattr__(\"item_input\", item_inputs_list)\n",
    "    model.__setattr__(\"item_embedding\",\n",
    "                      get_item_embedding(pooling_item_embedding_weight, item_features[item_feature_name]))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 实际应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtubednn_u2i_dict(data, topk=20):\n",
    "    \"\"\"\n",
    "    使用YoutubeDNN进行新闻召回计算\n",
    "    \"\"\"\n",
    "    \n",
    "    sparse_feature = [\"click_article_id\", \"user_id\"]\n",
    "    SEQ_LEN = 30 # paddding\n",
    "    \n",
    "    user_profile_ = data[[\"user_id\"]].drop_duplicates('user_id')\n",
    "    item_profile_ = data[[\"click_article_id\"]].drop_duplicates('click_article_id')  \n",
    "    \n",
    "    # 类别编码\n",
    "    features = [\"click_article_id\", \"user_id\"]\n",
    "    feature_max_idx = {} # 存储相对应的类别数目\n",
    "    \n",
    "    for feature in features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feature] = lbe.fit_transform(data[feature])\n",
    "        feature_max_idx[feature] = data[feature].max() + 1\n",
    "    \n",
    "    user_profile = data[[\"user_id\"]].drop_duplicates('user_id')\n",
    "    item_profile = data[[\"click_article_id\"]].drop_duplicates('click_article_id')\n",
    "    \n",
    "    # 标签前和标签后的索引对比\n",
    "    user_index_2_rawid = dict(zip(user_profile[\"user_id\"], user_profile_[\"user_id\"]))\n",
    "    item_index_2_rawid = dict(zip(item_profile[\"click_article_id\"], item_profile_[\"click_article_id\"]))\n",
    "    \n",
    "    # 划分数据和整理数据集\n",
    "    train_set, test_set = gen_data_set(data, negsample=0)\n",
    "    \n",
    "    train_model_input, train_label = gen_model_input(train_set, user_profile, seq_max_len=SEQ_LEN)\n",
    "    test_model_input, test_label = gen_model_input(test_set, user_profile, seq_max_len=SEQ_LEN)\n",
    "    \n",
    "    # Embedding维度\n",
    "    embedding_dim = 16\n",
    "    # 用户特征\n",
    "    user_feature_columns = [SparseFeat(\"user_id\", feature_max_idx[\"user_id\"], embedding_dim=embedding_dim),\n",
    "                                   VarLenSparseFeat(SparseFeat(\"hist_article_id\", feature_max_idx[\"click_article_id\"]\n",
    "                                                               , embedding_dim=embedding_dim, embedding_name=\"click_article_id\"), SEQ_LEN, \"mean\", length_name=\"hist_len\"),]  # 变量可变，选择设置为固定大小的序列长度，可使用均值填补\n",
    "    # 物品特征\n",
    "    item_feature_columns = [SparseFeat(\"click_article_id\", feature_max_idx[\"click_article_id\"], embedding_dim=embedding_dim)]\n",
    "    \n",
    "    # trianing model\n",
    "    model = YoutubeDNN(user_feature_columns, item_feature_columns, num_sampled=5, user_dnn_hidden_units=(64, embedding_dim))\n",
    "    model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n",
    "    history = model.fit(train_model_input, train_label, batch_size = 256, epochs=1, verbose=1, validation_split=0.0) # 不设置验证集，全数据进行训练\n",
    "    \n",
    "    # 得到物品和用户向量\n",
    "    test_user_model_input = test_model_input\n",
    "    all_item_model_input = {\"click_article_id\":item_profile[\"click_article_id\"].values}\n",
    "    \n",
    "    user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "    item_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "    \n",
    "    user_embs = user_embedding_model.predict(test_model_input, batch_size=2*12)\n",
    "    item_embs = item_embedding_model.predict(all_item_model_input, batch_size=2**12)\n",
    "    \n",
    "    # L2范数归一化\n",
    "    user_embs = user_embs / np.linalg.norm(user_embs, axis=1, keepdims=True)\n",
    "    item_embs = item_embs / np.linalg.norm(item_embs, axis=1, keepdims=True)\n",
    "    \n",
    "    # 保存以字典的形式进行\n",
    "    raw_user_id_emb_dict = {user_index_2_rawid[k]: v for k, v in zip(user_profile[\"user_id\"], user_embs)}\n",
    "    raw_item_id_emb_dict = {item_index_2_rawid[k]: v for k, v in zip(item_profile[\"click_article_id\"], item_embs)}\n",
    "    pickle.dump(raw_user_id_emb_dict, open(\"./data/\" + \"user_youtube_emb.pkl\", \"wb\"))\n",
    "    pickle.dump(raw_item_id_emb_dict, open(\"./data/\" + \"item_youtube_emb.pkl\", \"wb\"))\n",
    "    \n",
    "    # faiss 搜索，对每一个用户搜索召回最近的K个商品(使用余项相似度)\n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    index.add(item_embs)\n",
    "    \n",
    "    # 搜索\n",
    "    sim, idx = index.search(np.ascontiguousarray(user_embs), topk)\n",
    "    user_recall_item_dict = collections.defaultdict(dict)\n",
    "    \n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(test_user_model_input[\"user_id\"], sim, idx)):\n",
    "        target_raw_idx = user_index_2_rawid[target_idx]\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):  # 首元素为自身\n",
    "            rele_raw_idx = item_index_2_rawid[rele_idx]\n",
    "            #　得到用户相关物品去与其相对应的相似度数值\n",
    "            user_recall_item_dict[target_raw_idx][rele_raw_idx] = user_recall_item_dict.get(target_idx, {}).get(rele_raw_idx, 0) + sim_value\n",
    "            \n",
    "    # 对召回结果进行排序\n",
    "    user_recall_item_dict = {k:sorted(v.items(), key=lambda x:x[1], reverse=True) for k, v in user_recall_item_dict.items()}\n",
    "    \n",
    "    pickle.dump(user_recall_item_dict, open(\"./data/\" + \"youtube_u2i_dict.pkl\", \"wb\"))\n",
    "    \n",
    "    return user_recall_item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:31<00:00, 8013.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/oem/anaconda3/envs/zwynn/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v1.py:47: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 1149673 samples\n",
      "1149673/1149673 [==============================] - 269s 234us/sample - loss: 0.1458\n",
      "WARNING:tensorflow:From /home/oem/anaconda3/envs/zwynn/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250000it [00:19, 13154.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# 召回数据\n",
    "metric_recall = False\n",
    "\n",
    "if not metric_recall:\n",
    "    user_multi_recall_dict[\"youtubednn_recall\"] = youtubednn_u2i_dict(all_click_df, topk=20)\n",
    "else:\n",
    "    trn_hist_click, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "    user_multi_recall_dict[\"youtubednn_recall\"] = youtubednn_u2i_dict(trn_hist_click, topk=20)\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['youtubednn_recall'], trn_last_click_df, topk=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ItemCF Recall的召回操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于商品的召回i2i\n",
    "def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim):\n",
    "    \"\"\"\n",
    "        基于文章协同过滤的召回\n",
    "        :param user_id: 用户id\n",
    "        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   {user1: [(item1, time1), (item2, time2)..]...}\n",
    "        :param i2i_sim: 字典，文章相似性矩阵\n",
    "        :param sim_item_topk: 整数， 选择与当前文章最相似的前k篇文章\n",
    "        :param recall_item_num: 整数， 最后的召回文章数量\n",
    "        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全\n",
    "        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵\n",
    "        \n",
    "        return: 召回的文章列表 [(item1, score1), (item2, score2)...]\n",
    "    \"\"\"\n",
    "    # 获取用户历史交互的文章\n",
    "    user_hist_items = user_item_time_dict[user_id]\n",
    "    user_hist_items_ = {user_id for user_id, _ in user_hist_items}\n",
    "    \n",
    "    item_rank = {}\n",
    "    for loc, (i, click_time) in enumerate(user_hist_items):\n",
    "        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n",
    "            if j in user_hist_items_:\n",
    "                continue\n",
    "            \n",
    "            # 文章创建时间差权重\n",
    "            created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "            # 相似文章和历史点击文章序列中历史文章所在的位置权重\n",
    "            loc_weight = (0.9 ** (len(user_hist_items) - loc))\n",
    "            \n",
    "            content_weight = 1.0\n",
    "            if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                content_weight += emb_i2i_sim[i][j]\n",
    "            if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                content_weight += emb_i2i_sim[j][i]\n",
    "                \n",
    "            item_rank.setdefault(j, 0)\n",
    "            item_rank[j] += created_time_weight * loc_weight * content_weight * wij\n",
    "    \n",
    "    # 不足10个，用热门商品补全\n",
    "    if len(item_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in item_rank.items(): # 填充的item应该不在原来的列表中\n",
    "                continue\n",
    "            item_rank[item] = - i - 100 # 随便给个负数就行\n",
    "            if len(item_rank) == recall_item_num:\n",
    "                break\n",
    "    \n",
    "    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n",
    "        \n",
    "    return item_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemcf_sim(df, item_created_time_dict):\n",
    "    \"\"\"\n",
    "        文章与文章之间的相似性矩阵计算\n",
    "        :param df: 数据表\n",
    "        :item_created_time_dict:  文章创建时间的字典\n",
    "        return : 文章与文章的相似性矩阵\n",
    "        \n",
    "        思路: 基于物品的协同过滤(详细请参考上一期推荐系统基础的组队学习) + 关联规则\n",
    "    \"\"\"\n",
    "    \n",
    "    user_item_time_dict = get_user_item_time(df)\n",
    "    \n",
    "    # 计算物品相似度\n",
    "    i2i_sim = {}\n",
    "    item_cnt = defaultdict(int)\n",
    "    for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "        # 在基于商品的协同过滤优化的时候可以考虑时间因素\n",
    "        for loc1, (i, i_click_time) in enumerate(item_time_list):\n",
    "            item_cnt[i] += 1\n",
    "            i2i_sim.setdefault(i, {})\n",
    "            for loc2, (j, j_click_time) in enumerate(item_time_list):\n",
    "                if(i == j):\n",
    "                    continue\n",
    "                    \n",
    "                # 考虑文章的正向顺序点击和反向顺序点击    \n",
    "                loc_alpha = 1.0 if loc2 > loc1 else 0.7\n",
    "                # 位置信息权重，其中的参数可以调节\n",
    "                loc_weight = loc_alpha * (0.9 ** (np.abs(loc2 - loc1) - 1))\n",
    "                # 点击时间权重，其中的参数可以调节\n",
    "                click_time_weight = np.exp(0.7 ** np.abs(i_click_time - j_click_time))\n",
    "                # 两篇文章创建时间的权重，其中的参数可以调节\n",
    "                created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                i2i_sim[i].setdefault(j, 0)\n",
    "                # 考虑多种因素的权重计算最终的文章之间的相似度\n",
    "                i2i_sim[i][j] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "                \n",
    "    i2i_sim_ = i2i_sim.copy()\n",
    "    for i, related_items in i2i_sim.items():\n",
    "        for j, wij in related_items.items():\n",
    "            i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "    \n",
    "    # 将得到的相似性矩阵保存到本地\n",
    "    pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "    \n",
    "    return i2i_sim_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [06:01<00:00, 690.65it/s] \n"
     ]
    }
   ],
   "source": [
    "i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [1:12:18<00:00, 57.62it/s]  \n"
     ]
    }
   ],
   "source": [
    "# 新闻召回，根据物品相似度\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = gen_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "\n",
    "i2i_sim = pickle.load(open(save_path + 'itemcf_i2i_sim.pkl', 'rb'))\n",
    "emb_i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl', 'rb'))\n",
    "\n",
    "sim_item_topk = 20  # 相似物品\n",
    "recall_item_num = 10 # 召回物品\n",
    "\n",
    "# 热度文章\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df[\"user_id\"].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "    \n",
    "user_multi_recall_dict[\"itemcf_sim_itemcf_recall\"] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict[\"itemcf_sim_itemcf_recall\"], open(save_path + \"itemcf_recall_dict.pkl\", \"wb\"))\n",
    "\n",
    "if metric_recall:\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['itemcf_sim_itemcf_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding CF 召回的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [01:32<00:00, 2696.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# 使用物品之间的Embedding相似度矩阵来计算相对应的物品召回\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = gen_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path + 'emb_i2i_sim.pkl', 'rb'))\n",
    "\n",
    "\n",
    "sim_item_topk = 20  # 相似物品\n",
    "recall_item_num = 10 # 召回物品\n",
    "\n",
    "# 热度文章\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "# 热度文章\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df[\"user_id\"].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "    \n",
    "user_multi_recall_dict[\"embedding_sim_item_recall\"] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict[\"embedding_sim_item_recall\"], open(save_path + \"embedding_sim_item_recall.pkl\", \"wb\"))\n",
    "\n",
    "if metric_recall:\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['embedding_sim_item_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UserCF的召回操作\n",
    "基于用户协同过滤，核心思想是给用户推荐与其相似的用户历史点击文章，因为这里涉及到了相似用户的历史文章，这里仍然可以加上一些关联规则来给用户可能点击的文章进行加权，这里使用的关联规则主要是考虑相似用户的历史点击文章与被推荐用户历史点击商品的关系权重，而这里的关系就可以直接借鉴基于物品的协同过滤相似的做法，只不过这里是对被推荐物品关系的一个累加的过程，下面是使用的一些关系权重，及相关的代码：\n",
    "\n",
    "1. <font color=\"red\">**算被推荐用户历史点击文章与相似用户历史点击文章的相似度，文章创建时间差，相对位置的总和，作为各自的权重**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于用户的召回 u2u2i\n",
    "def user_based_recommend(user_id, user_item_time_dict, u2u_sim, sim_user_topk, recall_item_num, \n",
    "                         item_topk_click, item_created_time_dict, emb_i2i_sim):\n",
    "    \"\"\"\n",
    "        基于文章协同过滤的召回\n",
    "        :param user_id: 用户id\n",
    "        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   {user1: [(item1, time1), (item2, time2)..]...}\n",
    "        :param u2u_sim: 字典，文章相似性矩阵\n",
    "        :param sim_user_topk: 整数， 选择与当前用户最相似的前k个用户\n",
    "        :param recall_item_num: 整数， 最后的召回文章数量\n",
    "        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全\n",
    "        :param item_created_time_dict: 文章创建时间列表\n",
    "        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵\n",
    "        \n",
    "        return: 召回的文章列表 [(item1, score1), (item2, score2)...]\n",
    "    \"\"\"\n",
    "    # 历史交互\n",
    "    user_item_time_list = user_item_time_dict[user_id]    #  [(item1, time1), (item2, time2)..]\n",
    "    user_hist_items = set([i for i, t in user_item_time_list])   # 存在一个用户与某篇文章的多次交互， 这里得去重\n",
    "    \n",
    "    items_rank = {}\n",
    "    for sim_u, wuv in sorted(u2u_sim[user_id].items(), key=lambda x: x[1], reverse=True)[:sim_user_topk]:\n",
    "        for i, click_time in user_item_time_dict[sim_u]:\n",
    "            if i in user_hist_items:\n",
    "                continue\n",
    "            items_rank.setdefault(i, 0)\n",
    "            \n",
    "            \n",
    "            # 考虑规则的召回\n",
    "            loc_weight = 1.0\n",
    "            content_weight = 1.0\n",
    "            created_time_weight = 1.0\n",
    "            \n",
    "            # 当前文章　　与　　该用户看的历史文章　　进行一个权重交互（权重加和）\n",
    "            for loc, (j, click_time) in enumerate(user_item_time_list):\n",
    "                # 点击时的相对位置权重（点击位置越靠近，权重越大，其以按照时间先后顺序进行排列）\n",
    "                loc_weight += 0.9 ** (len(user_item_time_list) - loc)\n",
    "                # 内容相似性权重\n",
    "                if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[i][j]\n",
    "                if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[j][i]\n",
    "                \n",
    "                # 创建时间差权重（时间差越大，其对应的权重值变小）\n",
    "                created_time_weight += np.exp(0.8 * np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "                \n",
    "            items_rank[i] += loc_weight * content_weight * created_time_weight * wuv\n",
    "        \n",
    "    # 热度补全\n",
    "    if len(items_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in items_rank.items(): # 填充的item应该不在原来的列表中\n",
    "                continue\n",
    "            items_rank[item] = - i - 100 # 随便给个复数就行\n",
    "            if len(items_rank) == recall_item_num:\n",
    "                break\n",
    "        \n",
    "    items_rank = sorted(items_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]    \n",
    "    \n",
    "    return items_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 此处无真实的的用户相似度字典，不进行计算\n",
    "# if metric_recall:\n",
    "#     trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "# else:\n",
    "#     trn_hist_click_df = all_click_df\n",
    "    \n",
    "# user_recall_items_dict = collections.defaultdict(dict)\n",
    "# user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "\n",
    "# u2u_sim = pickle.load(open(save_path + 'usercf_u2u_sim.pkl', 'rb'))\n",
    "\n",
    "# sim_user_topk = 20\n",
    "# recall_item_num = 10\n",
    "# item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "# for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "#     user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \\\n",
    "#                                                         recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)    \n",
    "\n",
    "# pickle.dump(user_recall_items_dict, open(save_path + 'usercf_u2u2i_recall.pkl', 'wb'))\n",
    "\n",
    "# if metric_recall:\n",
    "#     # 召回效果评估\n",
    "#     metrics_recall(user_recall_items_dict, trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User_embedding 召回\n",
    "\n",
    "**目的**：通过使用YoutubeDNN计算得到的用户embedding向量来计算用户相似度矩阵，从而得到基于规则的用户CF召回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u2u_embedding_sim(click_df, user_emb_dict , save_path, topk):\n",
    "    \"\"\"通过Embedding计算用户相似度\"\"\"\n",
    "    \n",
    "    user_list = []\n",
    "    user_emb_list = []\n",
    "    \n",
    "    for user_id, user_emb in user_emb_dict.items():\n",
    "        user_list.append(user_id)\n",
    "        user_emb_list.append(user_emb)\n",
    "        \n",
    "    # 存储装换索引对应关系\n",
    "    user_index_2_rawid_dict = {k:v for k, v in zip(range(len(user_list)), user_list)}\n",
    "    user_emb_np = np.array(user_emb_list)\n",
    "    \n",
    "    # faiss搜做\n",
    "    user_index = faiss.IndexFlatIP(user_emb_np.shape[1])\n",
    "    user_index.add(user_emb_np)\n",
    "    sim, idx = user_index.search(user_emb_np, topk)\n",
    "    \n",
    "    # 存储相似度\n",
    "    user_sim_dict = collections.defaultdict(dict)\n",
    "    \n",
    "    # 获取用户相似度\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(user_emb_np)), sim, idx)):\n",
    "        target_raw_idx = user_index_2_rawid_dict[target_idx]\n",
    "        \n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "            rele_raw_idx = user_index_2_rawid_dict[rele_idx]\n",
    "            \n",
    "            user_sim_dict[target_raw_idx][rele_raw_idx] = user_sim_dict.get(target_raw_idx, {}).get(rele_raw_idx, 0) + sim_value\n",
    "            \n",
    "    pickle.dump(user_sim_dict, open(save_path + \"youtube_u2u_sim.pkl\", \"wb\"))\n",
    "    \n",
    "    return user_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "250000it [00:09, 26516.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# 存储通过YoutubeDNN存放的用户embedding向量\n",
    "user_emb_dict = pickle.load(open(save_path + 'user_youtube_emb.pkl', 'rb'))\n",
    "\n",
    "# 计算相似度\n",
    "u2u_sim = u2u_embedding_sim(all_click_df, user_emb_dict, save_path, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [07:26<00:00, 559.66it/s] \n"
     ]
    }
   ],
   "source": [
    "# 使用召回评估函数验证当前召回方式的效果\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "u2u_sim = pickle.load(open(save_path + 'youtube_u2u_sim.pkl', 'rb'))\n",
    "\n",
    "sim_user_topk = 20\n",
    "recall_item_num = 10\n",
    "\n",
    "# 热门新闻\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique()):\n",
    "    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \\\n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "    \n",
    "user_multi_recall_dict['youtubednn_usercf_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['youtubednn_usercf_recall'], open(save_path + 'youtubednn_usercf_recall.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['youtubednn_usercf_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冷启动问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**冷启动问题可以分成三类：文章冷启动，用户冷启动，系统冷启动。**\n",
    "\n",
    "- 文章冷启动：对于一个平台系统新加入的文章，该文章没有任何的交互记录，如何推荐给用户的问题。(对于我们场景可以认为是，日志数据中没有出现过的文章都可以认为是冷启动的文章)\n",
    "- 用户冷启动：对于一个平台系统新来的用户，该用户还没有文章的交互信息，如何给该用户进行推荐。(对于我们场景就是，测试集中的用户是否在测试集对应的log数据中出现过，如果没有出现过，那么可以认为该用户是冷启动用户。但是有时候并没有这么严格，我们也可以自己设定某些指标来判别哪些用户是冷启动用户，比如通过使用时长，点击率，留存率等等)\n",
    "- 系统冷启动：就是对于一个平台刚上线，还没有任何的相关历史数据，此时就是系统冷启动，其实也就是前面两种的一个综合。\n",
    "\n",
    "**当前场景下冷启动问题的分析：**\n",
    "\n",
    "对当前的数据进行分析会发现，日志中所有出现过的点击文章只有3w多个，而整个文章库中却有30多万，那么测试集中的用户最后一次点击是否会点击没有出现在日志中的文章呢？如果存在这种情况，说明用户点击的文章之前没有任何的交互信息，这也就是我们所说的文章冷启动。通过数据分析还可以发现，测试集用户只有一次点击的数据占得比例还不少，其实仅仅通过用户的一次点击就给用户推荐文章使用模型的方式也是比较难的，这里其实也可以考虑用户冷启动的问题，但是这里只给出物品冷启动的一些解决方案及代码，关于用户冷启动的话提一些可行性的做法。\n",
    "\n",
    "1. 文章冷启动(没有冷启动的探索问题)    \n",
    "   其实我们这里不是为了做文章的冷启动而做冷启动，而是猜测用户可能会点击一些没有在log数据中出现的文章，我们要做的就是如何从将近27万的文章中选择一些文章作为用户冷启动的文章，这里其实也可以看成是一种召回策略，我们这里就采用简单的比较好理解的基于规则的召回策略来获取用户可能点击的未出现在log数据中的文章。\n",
    "   现在的问题变成了：如何给每个用户考虑从27万个商品中获取一小部分商品？随机选一些可能是一种方案。下面给出一些参考的方案。\n",
    "   1. 首先基于Embedding召回一部分与用户历史相似的文章\n",
    "   2. 从基于Embedding召回的文章中通过一些规则过滤掉一些文章，使得留下的文章用户更可能点击。<font color = \"red\">我们这里的规则，可以是，留下那些与用户历史点击文章主题相同的文章，或者字数相差不大的文章。并且留下的文章尽量是与测试集用户最后一次点击时间更接近的文章，或者是当天的文章也行。</font>\n",
    "2. 用户冷启动    \n",
    "   这里对测试集中的用户点击数据进行分析会发现，测试集中有百分之20的用户只有一次点击，那么这些点击特别少的用户的召回是不是可以单独做一些策略上的补充呢？或者是在排序后直接基于规则加上一些文章呢？这些都可以去尝试，这里没有提供具体的做法。\n",
    "   \n",
    "**注意：**   \n",
    "\n",
    "这里看似和基于embedding计算的item之间相似度然后做itemcf是一致的，但是现在我们的目的不一样，我们这里的目的是找到相似的向量，并且还没有出现在log日志中的商品，再加上一些其他的冷启动的策略，这里需要找回的数量会偏多一点，不然被筛选完之后可能都没有文章了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [01:43<00:00, 2410.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# 通过embedding相似度计算与之相对应的新闻推荐，由于embedding为总文章的向量，所以可以计算推荐未在训练样本中所出现的数据，即物品冷启动\n",
    "# 不做召回评估\n",
    "trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = collections.defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path + \"emb_i2i_sim.pkl\", \"rb\"))\n",
    "\n",
    "sim_item_topk = 150\n",
    "recall_item_num = 100 # 召回数目\n",
    "\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df[\"user_id\"].unique()):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk\n",
    "                                                        , recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "    \n",
    "pickle.dump(user_recall_items_dict, open(save_path + \"cold_start_items_raw_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于规则进行文章过滤\n",
    "# 保留文章主题与用户历史浏览主题相似的文章\n",
    "# 保留文章字数与用户历史浏览文章字数相差不大的文章\n",
    "# 保留最后一次点击当天的文章\n",
    "# 按照相似度返回最终的结果\n",
    "\n",
    "def get_click_article_ids_set(all_click_df):\n",
    "    return set(all_click_df.click_article_id.values)\n",
    "\n",
    "def cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                     user_last_item_created_time_dict, item_type_dict, item_words_dict, \n",
    "                     item_created_time_dict, click_article_ids_set, recall_item_num):\n",
    "    \"\"\"\n",
    "        冷启动的情况下召回一些文章\n",
    "        :param user_recall_items_dict: 基于内容embedding相似性召回来的很多文章， 字典， {user1: [(item1, item2), ..], }\n",
    "        :param user_hist_item_typs_dict: 字典， 用户点击的文章的主题映射\n",
    "        :param user_hist_item_words_dict: 字典， 用户点击的历史文章的字数映射\n",
    "        :param user_last_item_created_time_idct: 字典，用户点击的历史文章创建时间映射\n",
    "        :param item_tpye_idct: 字典，文章主题映射\n",
    "        :param item_words_dict: 字典，文章字数映射\n",
    "        :param item_created_time_dict: 字典， 文章创建时间映射\n",
    "        :param click_article_ids_set: 集合，用户点击过得文章, 也就是日志里面出现过的文章\n",
    "        :param recall_item_num: 召回文章的数量， 这个指的是没有出现在日志里面的文章数量\n",
    "    \"\"\"\n",
    "    \n",
    "    cold_start_user_items_dict = {}\n",
    "    for user, item_list in tqdm(user_recall_items_dict.items()):\n",
    "        cold_start_user_items_dict.setdefault(user, [])\n",
    "        for item, score in item_list:\n",
    "            # 获取历史文章信息\n",
    "            hist_item_type_set = user_hist_item_typs_dict[user]\n",
    "            hist_mean_words = user_hist_item_words_dict[user] #平均阅读字数\n",
    "            hist_last_item_created_time = user_last_item_created_time_dict[user]\n",
    "            hist_last_item_created_time = datetime.fromtimestamp(hist_last_item_created_time)\n",
    "            \n",
    "            # 获取当前召回文章的信息\n",
    "            curr_item_type = item_type_dict[item]\n",
    "            curr_item_words = item_words_dict[item]\n",
    "            curr_item_created_time = item_created_time_dict[item]\n",
    "            curr_item_created_time = datetime.fromtimestamp(curr_item_created_time)\n",
    "            \n",
    "            # 新闻不能出现在已有的日志记录中,eg:类型，点击文章，字数创建时间\n",
    "            if  curr_item_type not in hist_item_type_set or \\\n",
    "                item not in click_article_ids_set or \\\n",
    "                abs(curr_item_words - hist_mean_words) > 200 or \\\n",
    "                abs((curr_item_created_time-hist_last_item_created_time).days)>90:\n",
    "                continue\n",
    "            \n",
    "            cold_start_user_items_dict[user].append((item, score))\n",
    "        \n",
    "    # 控制冷启动所召回的数目\n",
    "    cold_start_user_items_dict = {k : sorted(v, key=lambda x:x[1], reverse=True)[:recall_item_num] for k, v in cold_start_user_items_dict.items()}\n",
    "    pickle.dump(cold_start_user_items_dict, open(save_path + \"cold_start_user_items_dict.pkl\", \"wb\"))\n",
    "\n",
    "    return cold_start_user_items_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:55<00:00, 4470.45it/s]\n"
     ]
    }
   ],
   "source": [
    "all_click_df_ = all_click_df.copy()\n",
    "all_click_df_ = all_click_df_.merge(item_info_df, how='left', on='click_article_id')\n",
    "\n",
    "user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict = get_user_hist_item_info_dict(all_click_df_)\n",
    "\n",
    "click_article_ids_set = get_click_article_ids_set(all_click_df)\n",
    "# 需要注意的是\n",
    "# 这里使用了很多规则来筛选冷启动的文章，所以前面再召回的阶段就应该尽可能的多召回一些文章，否则很容易被删掉\n",
    "cold_start_user_items_dict = cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                                              user_last_item_created_time_dict, item_type_dict, item_words_dict, \\\n",
    "                                              item_created_time_dict, click_article_ids_set, recall_item_num)\n",
    "\n",
    "user_multi_recall_dict['cold_start_recall'] = cold_start_user_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多路召回合并\n",
    "多路召回合并就是将前面所有的召回策略得到的用户文章列表合并起来，下面是对前面所有召回结果的汇总\n",
    "1. 基于itemcf计算的item之间的相似度sim进行的召回 \n",
    "2. 基于embedding搜索得到的item之间的相似度进行的召回\n",
    "3. YoutubeDNN召回\n",
    "4. YoutubeDNN得到的user之间的相似度进行的召回\n",
    "5. 基于冷启动策略的召回\n",
    "\n",
    "**注意：**  \n",
    "在做召回评估的时候就会发现有些召回的效果不错有些召回的效果很差，所以对每一路召回的结果，我们可以认为的定义一些权重，来做最终的相似度融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_recall_results(user_multi_recall_dict, weigh_dict=None, topk=25):\n",
    "    \"\"\"融合不同的召回策略\"\"\"\n",
    "    \n",
    "    final_recall_items_dict = {}\n",
    "    \n",
    "    def norm_user_recall_item_sim(sorted_item_dict):\n",
    "        \"\"\"归一化相似度，便于最后的计算\"\"\"\n",
    "        \n",
    "        # 召回数量为0或１\n",
    "        if len(sorted_item_dict) < 2:\n",
    "            return sorted_item_dict\n",
    "        \n",
    "        # 进行归一化\n",
    "        min_sim = sorted_item_list[-1][1]\n",
    "        max_sim = sorted_item_list[0][1]\n",
    "        norm_sorted_item_list = []\n",
    "        \n",
    "        for item, score in sorted_item_dict:\n",
    "            if max_sim > 0:\n",
    "                norm_score = 1.0*(score - min_sim)/(max_sim - min_sim)\n",
    "            else:\n",
    "                norm_score = 0.0\n",
    "            norm_sorted_item_list.append((item, norm_score))\n",
    "        \n",
    "        return norm_sorted_item_list\n",
    "    \n",
    "    print(\"多路召回合并......\")\n",
    "    for method, user_recall_items in tqdm(user_multi_recall_dict.items()):\n",
    "        print(method + \"......\")\n",
    "        \n",
    "        # 设置召回权重\n",
    "        if weight_dict is None:\n",
    "            recall_method_weight = 1.0\n",
    "        else:\n",
    "            recall_method_weight = weight_dict[method]\n",
    "        # 归一化\n",
    "        for user_id, sorted_item_list in user_recall_items.items():\n",
    "            user_recall_items[user_id] = norm_user_recall_item_sim(sorted_item_list)\n",
    "        # 计算召回\n",
    "        for user_id, sorted_item_list in user_recall_items.items():\n",
    "            final_recall_items_dict.setdefault(user_id, {})\n",
    "            for item, score in sorted_item_list:\n",
    "                final_recall_items_dict[user_id].setdefault(item, 0)\n",
    "                final_recall_items_dict[user_id][item] += recall_method_weight * score  #　不同召回方法下相应的权重值进行累加\n",
    "        \n",
    "    final_recall_items_dict_rank = {}\n",
    "    # 多路召回时也可以控制最终的召回数量\n",
    "    for user, recall_item_dict in final_recall_items_dict.items():\n",
    "        final_recall_items_dict_rank[user] = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "\n",
    "    # 将多路召回后的最终结果字典保存到本地\n",
    "    pickle.dump(final_recall_items_dict_rank, open(os.path.join(save_path, 'final_recall_items_dict.pkl'),'wb'))\n",
    "\n",
    "    return final_recall_items_dict_rank       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里直接对多路召回的权重给了一个相同的值，其实可以根据前面召回的情况来调整参数的值\n",
    "weight_dict = {'itemcf_sim_itemcf_recall': 1.0,\n",
    "               'embedding_sim_item_recall': 1.0,\n",
    "               'youtubednn_recall': 1.0,\n",
    "               'youtubednn_usercf_recall': 1.0, \n",
    "               'cold_start_recall': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多路召回合并......\n",
      "itemcf_sim_itemcf_recall......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:05<00:23,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_sim_item_recall......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:10<00:16,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtubednn_recall......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:24<00:16,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtubednn_usercf_recall......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:30<00:07,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold_start_recall......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:57<00:00, 11.52s/it]\n"
     ]
    }
   ],
   "source": [
    "# 最终合并之后每个用户召回150个商品进行排序\n",
    "final_recall_items_dict_rank = combine_recall_results(user_multi_recall_dict, weight_dict, topk=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文件\n",
    "[YoutubeDNN召回理论加解释](https://mp.weixin.qq.com/s?src=11&timestamp=1608077602&ver=2769&signature=IA7HUMD9kNrVGxYC6MFdo2oBVjzhJoCiPYLF2KkXAZif7Tr1Vq5lQ3ZMAkoQac0fghXy*WOO38QvJND71nK0k6JBYg8892c4cx2tncaTV-yq-HVwbm5EF7VgnAfKW-kN&new=1)  \n",
    "[(Yourubednn运行时的一个错误)SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors](https://blog.csdn.net/weixin_42295205/article/details/110870741?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-2&spm=1001.2101.3001.4242)  \n",
    "[推荐系统召回策略之多路召回与Embedding召回](https://juejin.cn/post/6854573221707317261)  \n",
    "[推荐系统 embedding 技术实践总结](https://zhuanlan.zhihu.com/p/143763320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
