{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "SEED = 42\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    \"\"\"\n",
    "    From keras sorucecode: https://github.com/keras-team/keras/blob/master/keras/utils/np_utils.py#L9\n",
    "    \"\"\"\n",
    "\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras import backend as K\n",
    "# from keras.utils import to_categorical\n",
    "\n",
    "def data_preparation():\n",
    "    # The column names are from\n",
    "    # https://www2.1010data.com/documentationcenter/prod/Tutorials/MachineLearningExamples/CensusIncomeDataSet.html\n",
    "    column_names = ['age', 'class_worker', 'det_ind_code', 'det_occ_code', 'education', 'wage_per_hour', 'hs_college',\n",
    "                    'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member',\n",
    "                    'unemp_reason', 'full_or_part_emp', 'capital_gains', 'capital_losses', 'stock_dividends',\n",
    "                    'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ',\n",
    "                    'instance_weight', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',\n",
    "                    'num_emp', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',\n",
    "                    'own_or_self', 'vet_question', 'vet_benefits', 'weeks_worked', 'year', 'income_50k']\n",
    "\n",
    "    # Load the dataset in Pandas\n",
    "    train_df = pd.read_csv(\n",
    "        'data/census-income.data.gz',\n",
    "        delimiter=',',\n",
    "        header=None,\n",
    "        index_col=None,\n",
    "        names=column_names\n",
    "    )\n",
    "    other_df = pd.read_csv(\n",
    "        'data/census-income.test.gz',\n",
    "        delimiter=',',\n",
    "        header=None,\n",
    "        index_col=None,\n",
    "        names=column_names\n",
    "    )\n",
    "\n",
    "    # First group of tasks according to the paper\n",
    "    label_columns = ['income_50k', 'marital_stat']\n",
    "\n",
    "    # One-hot encoding categorical columns\n",
    "    categorical_columns = ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'major_ind_code',\n",
    "                           'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason',\n",
    "                           'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat',\n",
    "                           'det_hh_summ', 'mig_chg_msa', 'mig_chg_reg', 'mig_move_reg', 'mig_same', 'mig_prev_sunbelt',\n",
    "                           'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship',\n",
    "                           'vet_question']\n",
    "    train_raw_labels = train_df[label_columns]\n",
    "    other_raw_labels = other_df[label_columns]\n",
    "    transformed_train = pd.get_dummies(train_df.drop(label_columns, axis=1), columns=categorical_columns)\n",
    "    transformed_other = pd.get_dummies(other_df.drop(label_columns, axis=1), columns=categorical_columns)\n",
    "\n",
    "    # Filling the missing column in the other set\n",
    "    transformed_other['det_hh_fam_stat_ Grandchild <18 ever marr not in subfamily'] = 0\n",
    "\n",
    "    # One-hot encoding categorical labels\n",
    "    train_income = to_categorical((train_raw_labels.income_50k == ' 50000+.').astype(int), num_classes=2)\n",
    "    train_marital = to_categorical((train_raw_labels.marital_stat == ' Never married').astype(int), num_classes=2)\n",
    "    other_income = to_categorical((other_raw_labels.income_50k == ' 50000+.').astype(int), num_classes=2)\n",
    "    other_marital = to_categorical((other_raw_labels.marital_stat == ' Never married').astype(int), num_classes=2)\n",
    "\n",
    "    dict_outputs = {\n",
    "        'income': train_income.shape[1],\n",
    "        'marital': train_marital.shape[1]\n",
    "    }\n",
    "    dict_train_labels = {\n",
    "        'income': train_income,\n",
    "        'marital': train_marital\n",
    "    }\n",
    "    dict_other_labels = {\n",
    "        'income': other_income,\n",
    "        'marital': other_marital\n",
    "    }\n",
    "    output_info = [(dict_outputs[key], key) for key in sorted(dict_outputs.keys())]\n",
    "\n",
    "    # Split the other dataset into 1:1 validation to test according to the paper\n",
    "    validation_indices = transformed_other.sample(frac=0.5, replace=False, random_state=SEED).index\n",
    "    test_indices = list(set(transformed_other.index) - set(validation_indices))\n",
    "    validation_data = transformed_other.iloc[validation_indices]\n",
    "    validation_label = [dict_other_labels[key][validation_indices] for key in sorted(dict_other_labels.keys())]\n",
    "    test_data = transformed_other.iloc[test_indices]\n",
    "    test_label = [dict_other_labels[key][test_indices] for key in sorted(dict_other_labels.keys())]\n",
    "    train_data = transformed_train\n",
    "    train_label = [dict_train_labels[key] for key in sorted(dict_train_labels.keys())]\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label, output_info\n",
    "\n",
    "train_data, train_label, validation_data, validation_label, test_data, test_label, output_info = data_preparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTensorDataset(my_x, my_y):\n",
    "    tensor_x = torch.Tensor(my_x)\n",
    "    tensor_y = torch.Tensor(my_y)\n",
    "    return torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "\n",
    "train_label_tmp = np.column_stack((np.argmax(train_label[0], axis=1), np.argmax(train_label[1], axis=1)))\n",
    "train_loader = DataLoader(dataset=getTensorDataset(train_data.to_numpy(), train_label_tmp), batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_label_tmp = np.column_stack((np.argmax(validation_label[0], axis=1), np.argmax(validation_label[1], axis=1)))\n",
    "val_loader = DataLoader(dataset=getTensorDataset(validation_data.to_numpy(), validation_label_tmp), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # self.log_soft = nn.LogSoftmax(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.log_soft(out)\n",
    "        return out\n",
    "    \n",
    "class Tower(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super(Tower, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        # out = self.softmax(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMOE(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_experts, experts_out, experts_hidden, towers_hidden, tasks):\n",
    "        super(MMOE, self).__init__()\n",
    "        # params\n",
    "        self.input_size = input_size\n",
    "        self.num_experts = num_experts\n",
    "        self.experts_out = experts_out\n",
    "        self.experts_hidden = experts_hidden\n",
    "        self.towers_hidden = towers_hidden\n",
    "        self.tasks = tasks\n",
    "        # row by row\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # model\n",
    "        self.experts = nn.ModuleList([Expert(self.input_size, self.experts_out, self.experts_hidden) for i in range(self.num_experts)])\n",
    "        self.w_gates = nn.ParameterList([nn.Parameter(torch.randn(input_size, num_experts), requires_grad=True) for i in range(self.tasks)])\n",
    "        self.towers = nn.ModuleList([Tower(self.experts_out, 1, self.towers_hidden) for i in range(self.tasks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the experts output\n",
    "        expers_o = [e(x) for e in self.experts]\n",
    "        expers_o_tensor = torch.stack(expers_o)\n",
    "        \n",
    "        # get the gates output\n",
    "        gates_o = [self.softmax(x @ g) for g in self.w_gates]\n",
    "        \n",
    "        # multiply the output of the experts with the corresponding gates output\n",
    "        # res = gates_o[0].t().unsqueeze(2).expand(-1, -1, self.experts_out) * expers_o_tensor\n",
    "        # https://discuss.pytorch.org/t/element-wise-multiplication-of-the-last-dimension/79534\n",
    "        towers_input = [g.t().unsqueeze(2).expand(-1, -1, self.experts_out) * expers_o_tensor for g in gates_o]\n",
    "        towers_input = [torch.sum(ti, dim=0) for ti in towers_input]\n",
    "        \n",
    "        # get the final output from the towers\n",
    "        final_output = [t(ti) for t, ti in zip(self.towers, towers_input)]\n",
    "        \n",
    "        # get the output of the towers, and stack them\n",
    "        # final_output = torch.stack(final_output, dim=1)\n",
    "        \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_input = torch.tensor([[10.0, 10.0, 15.0, 30.0, 18.0], [20.0, 50.0, 28.0, 22.0, 12.0], [20.0, 50.0, 28.0, 22.0, 12.0]])\n",
    "mmoe = MMOE(input_size=5, num_experts=3, experts_out=4, experts_hidden=2, towers_hidden=2, tasks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.4259],\n",
       "         [0.4788],\n",
       "         [0.4788]], grad_fn=<SigmoidBackward>), tensor([[0.3621],\n",
       "         [0.3043],\n",
       "         [0.3043]], grad_fn=<SigmoidBackward>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmoe(simple_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MMOE(input_size=499, num_experts=6, experts_out=16, experts_hidden=32, towers_hidden=8, tasks=2)\n",
    "model = model.to(device)\n",
    "# print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 1/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 2/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 3/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 4/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 5/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 6/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 7/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 8/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 9/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 10/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 11/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 12/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 13/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 14/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 15/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 16/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 17/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 18/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 19/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 20/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 21/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 22/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 23/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 24/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 25/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 26/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 27/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 28/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 29/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 30/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 31/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 32/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 33/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 34/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 35/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 36/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 37/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 38/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 39/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 40/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 41/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 42/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 43/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 44/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 45/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 46/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 47/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 48/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 49/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 50/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 51/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 52/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 53/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 54/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 55/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 56/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 57/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 58/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 59/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 60/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 61/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 62/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 63/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 64/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 65/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 66/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 67/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 68/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 69/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 70/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 71/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 72/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 73/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 74/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 75/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 76/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 77/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 78/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 79/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 80/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 81/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 82/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 83/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 84/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 85/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 86/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 87/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 88/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 89/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 90/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 91/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 92/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 93/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 94/100\n",
      "    Batch: 0/194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 95/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 96/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 97/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 98/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "Epoch: 99/100\n",
      "    Batch: 0/194\n",
      "    Batch: 50/194\n",
      "    Batch: 100/194\n",
      "    Batch: 150/194\n",
      "0.41725474498822135\n",
      "0.4159550314168541\n"
     ]
    }
   ],
   "source": [
    "# Sets hyper-parameters\n",
    "lr = 1e-4\n",
    "n_epochs = 100\n",
    "\n",
    "# Defines loss function and optimizer\n",
    "# loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "loss_fn = nn.BCELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    # Uses loader to fetch one mini-batch for training\n",
    "    epoch_loss = []\n",
    "    c = 0\n",
    "    print(\"Epoch: {}/{}\".format(epoch, n_epochs))\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # NOW, sends the mini-batch data to the device\n",
    "        # so it matches location of the MODEL\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        # One stpe of training\n",
    "        yhat = model(x_batch)\n",
    "        \n",
    "        # loss = loss_fn(yhat, y_batch)\n",
    "        \n",
    "        y_batch_t1, y_batch_t2 = y_batch[:, 0], y_batch[:, 1]\n",
    "        yhat_t1, yhat_t2 = yhat[0], yhat[1]\n",
    "        \n",
    "        loss_t1 = loss_fn(yhat_t1, y_batch_t1.view(-1, 1))\n",
    "        loss_t2 = loss_fn(yhat_t2, y_batch_t2.view(-1, 1))\n",
    "        loss = loss_t1 + loss_t2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        if c % 50 == 0:\n",
    "            print(\"    Batch: {}/{}\".format(c, int(len(train_data)/BATCH_SIZE)))\n",
    "        c += 1\n",
    "    losses.append(np.mean(epoch_loss))\n",
    "        \n",
    "    # After finishing training steps for all mini-batches,\n",
    "    # it is time for evaluation!\n",
    "        \n",
    "    # We tell PyTorch to NOT use autograd...\n",
    "    with torch.no_grad():\n",
    "        # Uses loader to fetch one mini-batch for validation\n",
    "        epoch_loss = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            # Again, sends data to same device as model\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            \n",
    "            model.eval()\n",
    "            # Makes predictions\n",
    "            yhat = model(x_val)\n",
    "            # Computes validation loss\n",
    "            # val_loss = loss_fn(yhat, y_val)\n",
    "            \n",
    "            y_val_t1, y_val_t2 = y_val[:, 0], y_val[:, 1]\n",
    "            yhat_t1, yhat_t2 = yhat[0], yhat[1]\n",
    "\n",
    "            loss_t1 = loss_fn(yhat_t1, y_val_t1.view(-1, 1))\n",
    "            loss_t2 = loss_fn(yhat_t2, y_val_t2.view(-1, 1))\n",
    "            loss = loss_t1 + loss_t2\n",
    "            \n",
    "            epoch_loss.append(loss.item())\n",
    "    val_losses.append(np.mean(epoch_loss))\n",
    "\n",
    "# print(model.state_dict())\n",
    "print(np.mean(losses))\n",
    "print(np.mean(val_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, n_epochs+1)\n",
    "plt.plot(epochs, losses, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_losses, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_tmp = np.column_stack((np.argmax(test_label[0], axis=1), np.argmax(test_label[1], axis=1)))\n",
    "test_loader = DataLoader(dataset=getTensorDataset(test_data.to_numpy(), test_label_tmp), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5134528784119353\n"
     ]
    }
   ],
   "source": [
    "t1_pred = []\n",
    "t2_pred = []\n",
    "t1_target = []\n",
    "t2_target = []\n",
    "\n",
    "# We tell PyTorch to NOT use autograd...\n",
    "with torch.no_grad():\n",
    "    # Uses loader to fetch one mini-batch for testing\n",
    "    epoch_loss = []\n",
    "    for x_test, y_test in test_loader:\n",
    "        # Again, sends data to same device as model\n",
    "        x_test = x_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        # Makes predictions\n",
    "        yhat = model(x_test)\n",
    "\n",
    "        y_test_t1, y_test_t2 = y_test[:, 0], y_test[:, 1]\n",
    "        yhat_t1, yhat_t2 = yhat[0], yhat[1]\n",
    "\n",
    "        loss_t1 = loss_fn(yhat_t1, y_test_t1.view(-1, 1))\n",
    "        loss_t2 = loss_fn(yhat_t2, y_test_t2.view(-1, 1))\n",
    "        loss = loss_t1 + loss_t2\n",
    "        \n",
    "        # predict\n",
    "        t1_hat = yhat_t1.view(-1) > 0.5\n",
    "        t2_hat = yhat_t2.view(-1) > 0.5\n",
    "        \n",
    "        # save\n",
    "        t1_pred.append(t1_hat)\n",
    "        t2_pred.append(t2_hat)\n",
    "        t1_target.append(y_test_t1)\n",
    "        t2_target.append(y_test_t2)\n",
    "        \n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "print(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_pred = torch.cat(t1_pred)\n",
    "t2_pred = torch.cat(t2_pred)\n",
    "t1_target = torch.cat(t1_target)\n",
    "t2_target = torch.cat(t2_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46694,   103],\n",
       "       [ 2486,   598]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(t1_target.cpu().numpy(), t1_pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      1.00      0.97     46797\n",
      "         1.0       0.85      0.19      0.32      3084\n",
      "\n",
      "    accuracy                           0.95     49881\n",
      "   macro avg       0.90      0.60      0.64     49881\n",
      "weighted avg       0.94      0.95      0.93     49881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t1_target.cpu().numpy(), t1_pred.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.89      0.90     28284\n",
      "         1.0       0.86      0.89      0.87     21597\n",
      "\n",
      "    accuracy                           0.89     49881\n",
      "   macro avg       0.89      0.89      0.89     49881\n",
      "weighted avg       0.89      0.89      0.89     49881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(t2_target.cpu().numpy(), t2_pred.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing example for the loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5821)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.tensor([[[0.1, 0.5], [0.1, 0.5]], [[0.1, 0.5], [0.3, 0.5]], [[0.1, 0.5], [1.0, 1.0]]])\n",
    "target = torch.tensor([[0, 0], [1, 1], [1, 1]])\n",
    "output = loss(input, target)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
